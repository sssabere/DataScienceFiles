{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9959f48",
   "metadata": {
    "deletable": false
   },
   "source": [
    "\n",
    "# Assignment 4 for Course 1MS041\n",
    "Make sure you pass the `# ... Test` cells and\n",
    " submit your solution notebook in the corresponding assignment on the course website. You can submit multiple times before the deadline and your highest score will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d8b71",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "4",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "24"
   },
   "source": [
    "---\n",
    "## Assignment 4, PROBLEM 1\n",
    "Maximum Points = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1585d",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "4",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "24"
   },
   "source": [
    "\n",
    "    This time the assignment only consists of one problem, but we will do a more comprehensive analysis instead.\n",
    "\n",
    "Consider the dataset `Corona_NLP_train.csv` that you can get from the course website [git](https://github.com/datascience-intro/1MS041-2024/blob/main/notebooks/data/Corona_NLP_train.csv). The data is \"Coronavirus tweets NLP - Text Classification\" that can be found on [kaggle](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification). The data has several columns, but we will only be working with `OriginalTweet`and `Sentiment`.\n",
    "\n",
    "1. [3p] Load the data and filter out those tweets that have `Sentiment`=`Neutral`. Let $X$ represent the `OriginalTweet` and let \n",
    "    $$\n",
    "        Y = \n",
    "        \\begin{cases}\n",
    "        1 & \\text{if sentiment is towards positive}\n",
    "        \\\\\n",
    "        0 & \\text{if sentiment is towards negative}.\n",
    "        \\end{cases}\n",
    "    $$\n",
    "    Put the resulting arrays into the variables $X$ and $Y$. Split the data into three parts, train/test/validation where train is 60% of the data, test is 15% and validation is 25% of the data. Do not do this randomly, this is to make sure that we all did the same splits (we are in this case assuming the data is IID as presented in the dataset). That is [train,test,validation] is the splitting layout.\n",
    "\n",
    "2. [4p] There are many ways to solve this classification problem. The first main issue to resolve is to convert the $X$ variable to something that you can feed into a machine learning model. For instance, you can first use [`CountVectorizer`](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) as the first step. The step that comes after should be a `LogisticRegression` model, but for this to work you need to put together the `CountVectorizer` and the `LogisticRegression` model into a [`Pipeline`](https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline). Fill in the variable `model` such that it accepts the raw text as input and outputs a number $0$ or $1$, make sure that `model.predict_proba` works for this. **Hint: You might need to play with the parameters of LogisticRegression to get convergence, make sure that it doesn't take too long or the autograder might kill your code**\n",
    "3. [3p] Use your trained model and calculate the precision and recall on both classes. Fill in the corresponding variables with the answer.\n",
    "4. [3p] Let us now define a cost function\n",
    "    * A positive tweet that is classified as negative will have a cost of 1\n",
    "    * A negative tweet that is classified as positive will have a cost of 5\n",
    "    * Correct classifications cost 0\n",
    "    \n",
    "    complete filling the function `cost` to compute the cost of a prediction model under a certain prediction threshold (recall our precision recall lecture and the `predict_proba` function from trained models). \n",
    "\n",
    "5. [4p] Now, we wish to select the threshold of our classifier that minimizes the cost, fill in the selected threshold value in value `optimal_threshold`.\n",
    "6. [4p] With your newly computed threshold value, compute the cost of putting this model in production by computing the cost using the validation data. Also provide a confidence interval of the cost using Hoeffdings inequality with a 99% confidence.\n",
    "7. [3p] Let $t$ be the threshold you found and $f$ the model you fitted (one of the outputs of `predict_proba`), if we define the random variable\n",
    "    $$\n",
    "        C = (1-1_{f(X)\\geq t})Y+5(1-Y)1_{f(X) \\geq t}\n",
    "    $$\n",
    "    then $C$ denotes the cost of a randomly chosen tweet. In the previous step we estimated $\\mathbb{E}[C]$ using the empirical mean. However, since the threshold is chosen to minimize cost it is likely that $C=0$ or $C=1$ than $C=5$ as such it will have a low variance. Compute the empirical variance of $C$ on the validation set. What would be the confidence interval if we used Bennett's inequality instead of Hoeffding in point 6 but with the computed empirical variance as our guess for the variance?"
   ]
  },
  {
   "cell_type": "code",
   "id": "49e6fa33",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "4",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "24",
    "ExecuteTime": {
     "end_time": "2025-01-16T21:45:32.214466Z",
     "start_time": "2025-01-16T21:45:31.102349Z"
    }
   },
   "source": [
    "\n",
    "# Part 1\n",
    "\n",
    "# Load the data from the file specified in the problem definition and make sure that it is loaded using\n",
    "# the search path `data/Corona_NLP_train.csv`. This is to make sure the autograder and your computer have the same\n",
    "# file path and can load the data correctly.\n",
    "\n",
    "# Contrary to how many other problems are structured, this problem actually requires you to\n",
    "# have X on the shape (n_samples, ) that is a 1-dimensional array. Otherwise it will cause a bunch\n",
    "# of errors in the autograder or also in for instance CountVectorizer.\n",
    "\n",
    "# Make sure that all your data is numpy arrays and not pandas dataframes or series.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score\n",
    "recall_score\n",
    "\n",
    "data = pd.read_csv('data/Corona_NLP_train.csv', encoding='Latin1')\n",
    "data = data[data['Sentiment'] != 'Neutral']\n",
    "\n",
    "X = data['OriginalTweet']\n",
    "X = X.to_numpy()\n",
    "Y = data['Sentiment']\n",
    "Y = Y.apply(lambda x: 1 if x in['Positive', 'Extremely Positive'] else 0)\n",
    "Y = Y.to_numpy()\n",
    "\n",
    "n = len(Y)\n",
    "train_end = int(0.6 * n)\n",
    "test_end = train_end + int(0.15 * n)\n",
    "\n",
    "Y_train, Y_test, Y_valid = Y[:train_end], Y[train_end:test_end], Y[test_end:]\n",
    "X_train, X_test, X_valid = X[:train_end], X[train_end:test_end], X[test_end:]"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Corona_NLP_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m precision_score\n\u001B[1;32m     19\u001B[0m recall_score\n\u001B[0;32m---> 21\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata/Corona_NLP_train.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mLatin1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m data \u001B[38;5;241m=\u001B[39m data[data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSentiment\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNeutral\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     24\u001B[0m X \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOriginalTweet\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/ITDS/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ITDS/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/PycharmProjects/ITDS/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ITDS/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/PycharmProjects/ITDS/.venv/lib/python3.9/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/Corona_NLP_train.csv'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "078fe203",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "4",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities: [[0.87765727 0.12234273]\n",
      " [0.99712784 0.00287216]\n",
      " [0.03466503 0.96533497]\n",
      " ...\n",
      " [0.5880236  0.4119764 ]\n",
      " [0.94842033 0.05157967]\n",
      " [0.92857117 0.07142883]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 2\n",
    "\n",
    "# Train a machine learning model or pipeline that can take the raw strings from X and predict Y=0,1 depending on the\n",
    "# sentiment of the tweet. Store the trained model in the variable `model`.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "probabilities = model.predict_proba(X_test)\n",
    "print(\"Predicted Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6fd1d4",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "4",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8708971553610503 0.8808557727775729\n"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "\n",
    "# Evaluate the model on the test set and calculate precision, and recall on both classes. Store the results in the\n",
    "# variables `precision_0`, `precision_1`, `recall_0`, `recall_1`.\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "TP_1 = np.sum((Y_pred == 1) & (Y_test == 1))\n",
    "FP_1 = np.sum((Y_pred == 1) & (Y_test == 0))\n",
    "FN_1 = np.sum((Y_pred == 0) & (Y_test == 1))\n",
    "TP_0 = np.sum((Y_pred == 0) & (Y_test == 0))\n",
    "FP_0 = np.sum((Y_pred == 0) & (Y_test == 1))\n",
    "FN_0 = np.sum((Y_pred == 1) & (Y_test == 0))\n",
    "\n",
    "precision_0 = precision_score(Y_test, Y_pred)\n",
    "precision_1 = TP_1 / (TP_1 + FP_1) if (TP_1 + FP_1) > 0 else 0\n",
    "recall_0 = TP_0 / (TP_0 + FN_0) if (TP_0 + FN_0) > 0 else 0\n",
    "recall_1 = recall_score(Y_test, Y_pred)\n",
    "\n",
    "print(precision_1, recall_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10698374",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "4",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "24"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Part 4\n",
    "\n",
    " # Hint, make sure that the model has a predict_proba method\n",
    "    # think about how the decision is made based on the probabilities\n",
    "    # and how the threshold can be used to make the decision.\n",
    "    # For reference take a look at the lecture notes \"Bayes classifier\"\n",
    "    # which contains how the decision is made based on the probabilities when the threshold is 0.5.\n",
    "    \n",
    "    # Fill in what is missing to compute the cost and return it\n",
    "    # Note that we are interested in average cost\n",
    "\n",
    "def cost(model,threshold,X,Y):\n",
    "\n",
    "   probabilities = model.predict_proba(X)[:, 1] \n",
    "   predictions = (probabilities >= threshold).astype(int)\n",
    "    \n",
    "   # Initialize cost list\n",
    "   costs = []\n",
    "    \n",
    "   for pred, true_label in zip(predictions, Y):\n",
    "      if true_label == 1 and pred == 0:  # Positive classified as negative\n",
    "            costs.append(1)\n",
    "      elif true_label == 0 and pred == 1:  # Negative classified as positive\n",
    "            costs.append(5)\n",
    "      else:  # Correct classification\n",
    "            costs.append(0)\n",
    "    \n",
    "   # Return the average cost\n",
    "   return np.mean(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b21963ec",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "4",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.2727272727272727)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 5\n",
    "\n",
    "# Find the optimal threshold for the model on the test set. Store the threshold in the variable `optimal_threshold`\n",
    "# and the cost at the optimal threshold in the variable `cost_at_optimal_threshold` evaluated on the test set.\n",
    "\n",
    "optimal_threshold = None\n",
    "cost_at_optimal_threshold = float('inf')\n",
    "\n",
    "for threshold in np.linspace(0, 1, 101): \n",
    "    current_cost = cost(model, threshold, X_test, Y_test)\n",
    "    if current_cost < cost_at_optimal_threshold:\n",
    "        cost_at_optimal_threshold = current_cost\n",
    "        optimal_threshold = threshold\n",
    "\n",
    "# Outputs\n",
    "optimal_threshold, cost_at_optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79a1c5e6",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "4",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2727272727272727\n",
      "Confidence interval around the true cost [0.25,0.29]\n"
     ]
    }
   ],
   "source": [
    "# Part 6\n",
    "\n",
    "cost_at_optimal_threshold_valid = cost(model, optimal_threshold, X_valid, Y_valid)\n",
    "print(cost_at_optimal_threshold)\n",
    "m = len(X_valid)\n",
    "alpha = 0.01\n",
    "delta = (1/np.sqrt(m))*np.sqrt((1/2)*np.log(2/alpha))\n",
    "print(\"Confidence interval around the true cost [%.2f,%.2f]\" % (cost_at_optimal_threshold-delta,cost_at_optimal_threshold+delta))\n",
    "cost_interval_valid = (cost_at_optimal_threshold-delta, cost_at_optimal_threshold+delta)\n",
    "\n",
    "assert(type(cost_interval_valid) == tuple)\n",
    "assert(len(cost_interval_valid) == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3bc0760",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "4",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2472672792865777, 0.30762389507362314) 0.6692569047766288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 7\n",
    "\n",
    "def random_cost(threshold, model, X, Y):\n",
    "    probabilities = model.predict_proba(X)[:, 1]\n",
    "    C = np.where(\n",
    "        probabilities < threshold,  \n",
    "        Y,                          \n",
    "        5 * (1 - Y)              \n",
    "    )\n",
    "    return C\n",
    "\n",
    "C = random_cost(optimal_threshold, model, X_valid, Y_valid)\n",
    "\n",
    "m = len(C)\n",
    "mean_C = np.mean(C)\n",
    "var_C = np.var(C, ddof=0)  # Population variance (not sample variance)\n",
    "    \n",
    "M = np.max(C) - np.min(C)  # C ranges from 0 to 5, so M = 5\n",
    "    \n",
    "ln_term = np.log(2 / alpha)\n",
    "epsilon = np.sqrt((2 * var_C * ln_term) / m) + (ln_term * M) / (3 * m)\n",
    "\n",
    "lower_bound = mean_C - epsilon\n",
    "upper_bound = mean_C + epsilon\n",
    "\n",
    "CI = (lower_bound, upper_bound)\n",
    "\n",
    "variance_of_C = var_C\n",
    "interval_of_C = CI\n",
    "print(interval_of_C, variance_of_C)\n",
    "\n",
    "assert(type(interval_of_C) == tuple)\n",
    "assert(len(interval_of_C) == 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "lx_assignment_number": 4,
  "lx_course_instance": "2024",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
